<<<<<<< Updated upstream
<<<<<<< Updated upstream
import numpy as np
import math
class GibbsSampler :
    def __init__(self, trainer, test_data):
        self.trainer = trainer
        self.test_data= test_data
        self.tags = ['ADJ', 'ADV', 'ADP', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X', '.']
        self.exclude = ['_start', '_count']
        self.iterations = 2
        self.minimal_prob = math.pow(10, -7)

    def random_on_distribution(self,sequence, n, distribution=None):
        if distribution == None :
            return np.random.choice(sequence, n, [1] * len(sequence))
        return np.random.choice(sequence, n, distribution)

    def random_position(self,max):
        return np.random.randint(0, max, size = 1)[0]

    def do_gibbs(self, sentence):
        len_of_sentence = len(sentence)
        X = [['' for j in range(len_of_sentence)] for i in range(self.iterations)]
        X[0] = self.random_on_distribution(self.tags, len_of_sentence)
        for i in range(1, self.iterations) :
            X[i] = X[i-1]
            position = self.random_position(len_of_sentence)
            distribution = self.distribution_given_other_variables(position, X[i], sentence)
            print distribution
            X[i][position] = self.random_on_distribution(self.tags, 1, distribution)[0]

    def distribution_given_other_variables(self, position, sample, sentence):
        distribution = []
        for tag in self.tags:
            product = self.trainer['initial_probability'][sample[0]] * 1.0 / self.trainer['initial_probability']['_count']
            product *= self.trainer['transition'][sample[0]][sample[1]] * 1.0 / self.trainer['transition'][sample[0]]['_count']
            for i in range(2, len(sentence)) :
                product *= self.trainer['double_transition'][sample[i-1]+"_"+sample[i-2]][sample[i]] * 1.0 / self.trainer['double_transition'][sample[i-1]+"_"+sample[i-2]]['_count']
            for i in range(0, len(sentence)) :
                product *= self.trainer['reverse_emission'][sample[i]].get(sentence[i],self.minimal_prob) * 1.0 / self.trainer['reverse_emission'][sample[i]]['_count']
            distribution.append(product)
        return distribution



#print GibbsSampler(None, None).do_gibbs()

=======
=======
>>>>>>> Stashed changes
import numpy as np
import math
class GibbsSampler :
    def __init__(self, trainer, test_data):
        self.trainer = trainer
        self.test_data= test_data
        self.tags = ['ADJ', 'ADV', 'ADP', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X', '.']
        self.exclude = ['_start', '_count']
        self.iterations = 2
        self.minimal_prob = math.pow(10, -7)

    def random_on_distribution(self,sequence, n, distribution=None):
        if distribution == None :
            return np.random.choice(sequence, n, [1] * len(sequence))
        return np.random.choice(sequence, n, distribution)

    def random_position(self,max):
        return np.random.randint(0, max, size = 1)[0]

    def do_gibbs(self, sentence):
        len_of_sentence = len(sentence)
        X = [['' for j in range(len_of_sentence)] for i in range(self.iterations)]
        X[0] = self.random_on_distribution(self.tags, len_of_sentence)
        for i in range(1, self.iterations) :
            X[i] = X[i-1]
            position = self.random_position(len_of_sentence)
            distribution = self.distribution_given_other_variables(position, X[i], sentence)
            print distribution
            X[i][position] = self.random_on_distribution(self.tags, 1, distribution)[0]

    def distribution_given_other_variables(self, position, sample, sentence):
        distribution = []
        for tag in self.tags:
            product = self.trainer['initial_probability'][sample[0]] * 1.0 / self.trainer['initial_probability']['_count']
            product *= self.trainer['transition'][sample[0]][sample[1]] * 1.0 / self.trainer['transition'][sample[0]]['_count']
            for i in range(2, len(sentence)) :
                product *= self.trainer['double_transition'][sample[i-1]+"_"+sample[i-2]][sample[i]] * 1.0 / self.trainer['double_transition'][sample[i-1]+"_"+sample[i-2]]['_count']
            for i in range(0, len(sentence)) :
                product *= self.trainer['reverse_emission'][sample[i]].get(sentence[i],self.minimal_prob) * 1.0 / self.trainer['reverse_emission'][sample[i]]['_count']
            distribution.append(product)
        return distribution



#print GibbsSampler(None, None).do_gibbs()

<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
